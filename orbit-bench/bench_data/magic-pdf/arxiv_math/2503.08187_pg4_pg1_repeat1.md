where $\alpha > 0$ . The FWI problem in equation 7 ensures that the estimated model, while satisfying the data (and the wave equation), is smooth along the direction $\theta$ .  

Figure 1, top row, shows the anisotropic regularization balls for only one term of the regularizer equation 4 and for different values of $[ \sigma ] _ { i } = \sigma$ and ${ [ \theta ] _ { i } } ^ { - } = \theta$ . We observe that for $\sigma = 0 . 5$ , the anisotropic regularization becomes equivalent to the standard isotropic regularization. When the value of $\sigma$ increases, reaching the maximum considered value of 0.9, the degree of regularization applied in the direction $\theta$ and its normal is maximally different, resulting in a needle-shaped ellipse. This shape favors models with elongated features aligned with $\theta$ , while allowing variations in the normal direction to it, as illustrated in the bottom row.  

# 3 Algorithm  

The augmented Lagrangian function of equation 7 is  

$$
\begin{array} { l } { \displaystyle \mathcal { L } ( \theta , \sigma , u _ { s } , m , \lambda , \nu ) = \frac { 1 } { 2 } \sum _ { s = 1 } ^ { n _ { s } } \| P u _ { s } - d _ { s } \| _ { 2 } ^ { 2 } + \alpha \mathcal { R } ( m , \theta , \sigma ) } \\ { \displaystyle \quad + \frac { \mu } { 2 } \sum _ { s = 1 } ^ { n _ { s } } \| A ( m ) u _ { s } - b _ { s } \| _ { 2 } ^ { 2 } - \lambda _ { s } ^ { T } ( A ( m ) u _ { s } - b _ { s } ) } \\ { \displaystyle \quad + \frac { \tau } { 2 } \| \theta - z \| _ { 2 } ^ { 2 } - \nu ^ { T } ( \theta - z ) , } \end{array}
$$  

where $z ( \theta ) = \mathrm { m i n } ( \mathrm { m a x } ( \theta , - { \textstyle { \frac { \pi } { 2 } } } ) , { \textstyle { \frac { \pi } { 2 } } } )$ , $\lambda _ { s }$ and $\nu$ are Lagrange multipliers associated to the constraints, and $\mu > 0$ and $\tau > 0$ are the penalty parameters. The regularization parameter $\alpha > 0$ is given, being typically determined by the discrepancy principle.  

This problem can be efficiently solved using the alternating direction method of multipliers (ADMM, [15]), resulting in the following iterative procedure starting from initial guess $m$ (typically encoding some prior information), (spatially invariant) $\sigma = 0 . 5$ and initial multipliers $\lambda _ { s } = \nu = 0$ :  

$$
\begin{array} { r l } & { \theta ^ { + } = \underset { \theta } { \arg \operatorname* { m i n } } \ \mathcal { L } ( \theta , \sigma , u _ { s } , m , \lambda , \nu ) } \\ & { \sigma ^ { + } = \underset { \sigma } { \arg \operatorname* { m i n } } \ \mathcal { L } ( \theta ^ { + } , \sigma , u _ { s } , m , \lambda , \nu ) } \\ & { u _ { s } ^ { + } = \underset { u } { \arg \operatorname* { m i n } } \ \mathcal { L } ( \theta ^ { + } , \sigma ^ { + } , u , m , \lambda , \nu ) } \\ & { m ^ { + } = \underset { m } { \arg \operatorname* { m i n } } \ \mathcal { L } ( \theta ^ { + } , \sigma ^ { + } , u _ { s } ^ { + } , m , \lambda , \nu ) } \\ & { \lambda _ { s } ^ { + } = \lambda _ { s } - \mu ( A ( m ^ { + } ) u _ { s } ^ { + } - b _ { s } ) } \\ & { \nu ^ { + } = \nu - \tau ( \theta ^ { + } - z ^ { + } ) . } \end{array}
$$  

Here, the updated variables at each iteration are denoted by a superscript â€œ $\cdot _ { + } , \cdot _ { }$ . Each step of the algorithm addresses a specific subproblem, as described more in details below.  

Subproblem equation 8a involves minimizing the regularization function $\mathcal { R }$ , as defined in equation 4, with respect to $\theta$ , given the current values of $m$ and $\sigma$ . This can can be efficiently achieved using a single iteration of the Gauss-Newton method. To ensure the stability of the $\theta$ update, it is essential to incorporate a smoothing term, as suggested by [11].  

Subproblem equation 8b involves minimizing $\mathcal { R }$ with respect to $\sigma$ , resulting in:  

$$
[ \sigma ] _ { i } = \frac { [ g _ { z ^ { \prime } } ] _ { i } ^ { 2 } } { [ g _ { x ^ { \prime } } ] _ { i } ^ { 2 } + [ g _ { z ^ { \prime } } ] _ { i } ^ { 2 } } ,
$$  

where $\left( \left[ g _ { x ^ { \prime } } \right] _ { i } , \left[ g _ { z ^ { \prime } } \right] _ { i } \right) ^ { T } = R ( [ \theta ] _ { i } ) [ \nabla m ] _ { i }$ represent the rotated gradient of the current model at the $i$ th pixel. This expression for equation 9 is justified by considering that, when $[ \theta ] _ { i }$ corresponds to the correct orientation angle, we typically expect $| [ \bar { g _ { x ^ { \prime } } } ] _ { i } | \ll | [ \bar { g _ { z ^ { \prime } } } ] _ { i } |$ . In this case, directly minimizing $[ g _ { x ^ { \prime } } ] _ { i } ^ { \dot { 2 } } + [ g _ { z ^ { \prime } } ] _ { i } ^ { 2 }$ would heavily penalize $[ g _ { z ^ { \prime } } ] _ { i }$ , which is undesirable because we aim to apply more smoothing along the direction defined by $[ \theta ] _ { i }$ . The weights $[ \sigma ] _ { i }$ in equation 9 balance the contributions of gradient components in the weighted sum $[ \sigma ] _ { i } ^ { 2 } [ g _ { x ^ { \prime } } ] _ { i } ^ { \bar { 2 } } + ( 1 - [ \sigma ] _ { i } ) ^ { 2 } [ g _ { z ^ { \prime } } ] _ { i } ^ { \bar { 2 } }$ , ensuring that the magnitudes of both terms are approximately equal before penalization. This adaptive weighting mechanism effectively enhances smoothing along the structural direction $[ \theta ] _ { i }$ while allowing variations across it, eventually enabling the recovery of dominating structures and details in the model $m$ . To maintain stability during the $\sigma$ update, the computed weights are smoothed using a convolution with a $5 \times 5$ averaging filter; alternatively, one could incorporate a smoothing term.  

A comprehensive analysis of the methods for solving subproblems equation 8c and equation 8d can be found in [14].  