that $I _ { t }$ is surjective. Suppose $Z \in L ^ { 2 } ( Y , t , d ^ { \prime } )$ so that $ Z _ { n } - Z  _ { 2 }  0$ for some $Z _ { n } \in \mathcal { E } ( Y , t , d ^ { \prime } )$ . By isğ¼ğ‘¡ometry we conclude thağ‘t $H _ { n } : = I _ { t } ^ { - 1 } ( Z _ { n } )$ is a Câ€–ağ‘ğ‘›ucâˆ’hğ‘yâ€–2seâ†’qu0ence conveğ‘rğ‘›giâˆˆng(tğ‘Œo  sğ‘¡,oğ‘‘m)e $H \in L ^ { 2 } ( X , t , d ^ { \prime } )$ . Finally $\begin{array} { r } { \| I _ { t } H - Z \| = \operatorname* { l i m } _ { n \to \infty } \| Z _ { n } - Z \| = 0 } \end{array}$ and therefore $I _ { t } ( H ) = Z$ .  

Equation (3.11) is evident in the case that $\gamma$ is a simple function. For general $\gamma \in L _ { t } ( X , d ^ { \prime } )$ , choose a sequence of simple $\gamma ^ { ( n ) }$ with $\| { \boldsymbol { \gamma } } ^ { ( n ) } - { \boldsymbol { \gamma } } \| _ { 2 } \to 0$ . By (3.9) and ItoÂ¯â€™s isomğ›¾etrâˆˆyğ¿w(eğ‘‹h, ağ‘‘v)e  

$$
\Big | \int _ { 0 } ^ { t } \gamma ^ { ( n ) } ( s ) \mathrm { d } X ( s ) - \int _ { 0 } ^ { t } \gamma ( s ) \mathrm { d } X ( s ) \Big | _ { 2 } \to 0 , \qquad \Big | \int _ { 0 } ^ { t } \gamma ^ { ( n ) } ( s ) \mathrm { d } Y ( s ) - \int _ { 0 } ^ { t } \gamma ( s ) \mathrm { d } Y ( s ) \Big | _ { 2 } \to 0
$$  

as $n  \infty$ . The claim then follows because the continuity of the operator $I _ { t }$ yields  

$$
I _ { t } \int _ { 0 } ^ { t } \gamma ( s ) \mathrm { d } X ( s ) = \operatorname* { l i m } _ { n \to \infty } I _ { t } \int _ { 0 } ^ { t } \gamma ^ { ( n ) } ( s ) \mathrm { d } X ( s ) = \operatorname* { l i m } _ { n \to \infty } \int _ { 0 } ^ { t } \gamma ^ { ( n ) } ( s ) \mathrm { d } Y ( s ) = \int _ { 0 } ^ { t } \gamma ( s ) \mathrm { d } Y ( s ) .
$$  

# 4 Filtering, smoothing, and prediction  

This section is devoted to optimal linear filtering, prediction and smoothing of partially observed polynomial processes. We let either $I : = \mathbb { N }$ or $I : = \mathbb { R } _ { + }$ and fix a probability space $( \Omega , { \mathcal { F } } , ( { \mathcal { F } } _ { t } ) _ { t \in I } , \mathbb { P } )$ as well as an $\mathbb { R } ^ { d }$ -valued adağ¼pteâˆ¶d=prâ„•ocessğ¼ $X = ( X ( t ) ) _ { t \in I }$ . If $I = \mathbb { R } _ { + }$ , we assume $( \mathcal F _ { t } ) _ { t \in I }$ (to ğ‘¡b)eğ‘¡âˆˆğ¼r,i gâ„™h)t-continuous. â„Suppose that the componenğ‘‹ts $X _ { m + 1 } ( t ) , \hdots , X _ { d } ( t )$ â„ar+e observable (whğ‘¡e)rğ‘¡âˆˆeğ¼as $X _ { 1 } ( t ) , \ldots , X _ { m } ( t )$ are not. We let the subscript stand foğ‘‹r tğ‘šh+1e(ğ‘¡o)b,sâ€¦er,vğ‘‹ağ‘‘b(lğ‘¡e)part of a vector $\boldsymbol { x } \in \mathbb { R } ^ { d }$ anğ‘‹d1(lğ‘¡e)t, $H : = ( \delta _ { m + i , j } ) _ { i = 1 , \dots , d - m ; ~ j = 1 , \dots , d }$ , i.e. $x _ { \scriptscriptstyle 0 } : = H x = ( x _ { m + 1 } , \ldots , x _ { d } )$ . For $\boldsymbol { \Sigma } \in \mathbb { R } ^ { d \times d }$ we set $\Sigma _ { : , \circ } : = \Sigma H ^ { \top } = \Sigma _ { 1 : d , m + 1 : d } , \Sigma _ { 0 : } : = H \Sigma = \Sigma _ { m + 1 : d , 1 : d }$ âˆ¶a=s wğ»eğ‘¥ll=as( $\Sigma _ { \circ } : = H \Sigma H ^ { \top } = \Sigma _ { m + 1 : d , m + 1 : d } .$ The sÎ£ubscr=ipÎ£t $\mathrm { ~  ~ u ~ }$ st=anÎ£ding for thÎ£e uno=bsğ»erÎ£va=blÎ£e part of a vector isÎ£treat=edğ»iÎ£nğ»the=saÎ£me manner.  

We suppose that $\mathbb { E } ( \| X ( t ) \| ^ { 2 } ) < \infty$ for $t \in I$ and consider the following general filtering problem for fixed $t \in I$ .(â€–Tğ‘‹he(ğ‘¡)gâ€–oa) i<s âˆo miniğ‘¡mâˆˆiseğ¼ the mean square error $\mathbb { E } ( \| X ( t ) - Y \| ^ { 2 } )$ over all random variables $Y$ âˆˆthğ¼at are measurable with respect to the observableğ”¼i(nâ€–fğ‘‹or(ğ‘¡m)aâˆ’tioğ‘Œnâ€–  

$$
\mathcal { G } _ { t } : = \sigma \big ( \{ X _ { \circ } ( s ) : s \in I , s \leq t \} \big ) .
$$  

We call the minimiser of (4.1) the optimal filter for $X$ . Regardless of any specific model the optimal filter is then given by the conditional mean $\widehat { X } ( t , t ) : = \mathbb { E } ( X ( t ) | \mathcal { G } _ { t } )$  

# 4.1 Discrete-time linear filtering problems  

Let $I = \mathbb { N }$ . For Gaussian state space models, the optimal filter can be computed recursively:  

Proposition 4.1 (KÃ¡lmÃ¡n filter). Suppose that $X$ is $a$ linear Gaussian state space model as in Definition 3.1 and set $C ( t ) : = B ( t ) B ( t ) ^ { \top }$ . Let $\widehat { X } ( 0 , - 1 ) : = \mathbb { E } ( X ( 0 ) ) , \widehat { \Sigma } ( 0 , - 1 ) : = \mathrm { C o v } ( X ( 0 ) )$ and  

$$
\begin{array} { r l } & { \widehat { X } ( t + 1 , t ) : = a ( t + 1 ) + A ( t + 1 ) \widehat { X } ( t , t ) , } \\ & { \qquad \widehat { X } ( t , t ) : = \widehat { X } ( t , t - 1 ) + \widehat { \Sigma } _ { : , 0 } ( t , t - 1 ) \widehat { \Sigma } _ { 0 } ( t , t - 1 ) ^ { + } \big ( X _ { 0 } ( t ) - \widehat { X } _ { 0 } ( t , t - 1 ) \big ) , } \\ & { \widehat { \Sigma } ( t + 1 , t ) : = A ( t + 1 ) \widehat { \Sigma } ( t , t ) A ( t + 1 ) ^ { \top } + C ( t + 1 ) , } \\ & { \qquad \widehat { \Sigma } ( t , t ) : = \widehat { \Sigma } ( t , t - 1 ) - \widehat { \Sigma } _ { : , 0 } ( t , t - 1 ) \widehat { \Sigma } _ { 0 } ( t , t - 1 ) ^ { + } \widehat { \Sigma } _ { 0 ; : } ( t , t - 1 ) } \end{array}
$$  