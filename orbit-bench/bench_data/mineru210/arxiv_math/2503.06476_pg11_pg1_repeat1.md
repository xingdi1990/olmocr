which implies

$$
\sum _ { k = 0 } ^ { N } \alpha _ { k } \left( | \Theta ( x ^ { k } ) | + \frac { \| t ^ { k } \| ^ { 2 } } { 2 } \right) \leq \frac { 1 } { \beta } ( \Phi _ { j } ( x ^ { 0 } ) - \hat { y _ { j } } ) , \quad : : \ \theta ( x ^ { k } ) < 0 .
$$

Since the right-hand side of the above inequality is finite and inequality holds for any positive integer $N$ , then we get

$$
\sum _ { k = 0 } ^ { \infty } \alpha _ { k } \bigg ( | \Theta ( x ^ { k } ) | + \frac { | | t ^ { k } | | ^ { 2 } } { 2 } \bigg ) < \infty .
$$

The above inequality implies the result of this lemma.

Theorem 4.1. Suppose that $\Phi$ is convex in component-wise sense (i.e., $\Phi$ is $\mathbb { R } ^ { m } -$ convex) and the Assumption 1 holds. Then any sequence produced by Algorithm 3.1 converges to a WPOS $x ^ { * } \in \mathbb { R } ^ { n }$ .

Proof. Since by Algorithm 3.1, $\{ \Phi ( x ^ { k } ) \}$ is a component-wise decreasing sequence, then by assumption, there exists $\tilde { x } \in \mathbb R ^ { n }$ such that

$$
\Phi ( { \tilde { x } } ) \leq \Phi ( x ^ { k } ) { \mathrm { ~ f o r ~ a l l ~ } } k = 0 , 1 , 2 \ldots .
$$

It is observed that $0 < \alpha _ { k } \le 1$ for all $k$ , so

$$
\begin{array} { l } { | x ^ { k + 1 } - x ^ { k } | | ^ { 2 } \leq \displaystyle \frac { 1 } { \alpha _ { k } } \| x ^ { k + 1 } - x ^ { k } \| ^ { 2 } \mathrm { f o r ~ a l l ~ } k = 0 , 1 , 2 , \ldots } \\ { \qquad \leq \displaystyle \frac { 1 } { \alpha _ { k } } \| \alpha _ { k } t ^ { k } \| ^ { 2 } = \alpha _ { k } \| t ^ { k } \| ^ { 2 } \mathrm { f o r ~ a l l ~ } k = 0 , 1 , 2 , \ldots \mathrm { ~ } ( \because \mathrm { ~ } x ^ { k + 1 } = x ^ { k } + \alpha _ { k } t ^ { k } ) . } \end{array}
$$

Therefore, by above inequality, (4.18), and Lemma (4.5) we obtained

$$
\sum _ { k = 0 } ^ { \infty } \| x ^ { k + 1 } - x ^ { k } \| ^ { 2 } \leq \sum _ { k = 0 } ^ { \infty } \alpha _ { k } \| t ^ { k } \| ^ { 2 } < \infty .
$$

Thus,

$$
\sum _ { k = 0 } ^ { \infty } \| x ^ { k + 1 } - x ^ { k } \| ^ { 2 } < \infty .
$$

Let us define $\ddot { L } = \{ x \in \mathbb { R } ^ { n } : \Phi ( x ) \leq \Phi ( x ^ { k } ) , \ k = 0 , 1 , 2 , . . . \}$ . By the component-wise convexity of $\Phi$ and Lemma 4.3, for any $x \in \tilde { L }$ we have

$$
\| x - x ^ { k + 1 } \| ^ { 2 } \leq \| x - x ^ { k } \| ^ { 2 } + \| x ^ { k } - x ^ { k + 1 } \| ^ { 2 } { \mathrm { ~ f o r ~ a l l ~ } } k = 0 , 1 , 2 . . . .
$$

As $\tilde { L }$ is non empty because $\tilde { x } \in \tilde { L }$ , by (4.19) and the above inequality, it follows that $\{ x ^ { k } \}$ is quasi-Fejer convergent to the set $\tilde { L }$ . Then by Theorem 2.4, $\{ x ^ { k } \}$ is bounded and hence $\{ x ^ { k } \}$ has an accumulation point. Let $x ^ { * }$ be one of them. Then by Lemma 4.4, ${ \boldsymbol { x } } ^ { * } \in { \tilde { \cal L } }$ . Then by Theorem 2.4, we observe that $\{ x ^ { k } \}$ converges to $x ^ { * }$ . Therefore, by Theorem 3.3, $x ^ { * }$ is a critical point, and hence $\mathbb { R } ^ { m } -$ convexity implies that $x ^ { * }$ is a weak Pareto optimal solution for $\Phi$ .