that $I _ { t }$ is surjective. Suppose $Z \in L ^ { 2 } ( Y , t , d ^ { \prime } )$ so that $ Z _ { n } - Z  _ { 2 }  0$ for some $Z _ { n } \in { \mathcal { E } } ( Y , t , d ^ { \prime } )$ . By isometry we conclude that $H _ { n } : = I _ { t } ^ { - 1 } ( Z _ { n } )$ is a Cauchy sequence converging to some $H \in L ^ { 2 } ( X , t , d ^ { \prime } )$ . Finally $\begin{array} { r } { \| I _ { t } H - Z \| = \operatorname* { l i m } _ { n \to \infty } \| Z _ { n } - Z \| = 0 } \end{array}$ and therefore $I _ { t } ( H ) = Z$ .

Equation (3.11) is evident in the case that $\gamma$ is a simple function. For general $\gamma \in L _ { t } ( X , d ^ { \prime } )$ , choose a sequence of simple $\gamma ^ { ( n ) }$ with $\| \gamma ^ { ( n ) } - \gamma \| _ { 2 } \to 0$ . By (3.9) and Ito’s isometry we have ¯

$$
\int _ { 0 } ^ { t } \gamma ^ { ( n ) } ( s ) \mathrm { d } X ( s ) - \int _ { 0 } ^ { t } \gamma ( s ) \mathrm { d } X ( s ) \Big \Vert _ { 2 } \to 0 , \qquad \Big \Vert \int _ { 0 } ^ { t } \gamma ^ { ( n ) } ( s ) \mathrm { d } Y ( s ) - \int _ { 0 } ^ { t } \gamma ( s ) \mathrm { d } Y ( s ) \Big \Vert _ { 2 } \to 0
$$

as $n \to \infty$ . The claim then follows because the continuity of the operator $I _ { t }$ yields

$$
I _ { t } \int _ { 0 } ^ { t } \gamma ( s ) \mathrm { d } X ( s ) = \operatorname* { l i m } _ { n \to \infty } I _ { t } \int _ { 0 } ^ { t } \gamma ^ { ( n ) } ( s ) \mathrm { d } X ( s ) = \operatorname* { l i m } _ { n \to \infty } \int _ { 0 } ^ { t } \gamma ^ { ( n ) } ( s ) \mathrm { d } Y ( s ) = \int _ { 0 } ^ { t } \gamma ( s ) \mathrm { d } Y ( s ) .
$$

# 4 Filtering, smoothing, and prediction

This section is devoted to optimal linear filtering, prediction and smoothing of partially observed polynomial processes. We let either $I : = \mathbb { N }$ or $I : = \mathbb { R } _ { + }$ and fix a probability space $( \Omega , \mathcal { F } , ( \mathcal { F } _ { t } ) _ { t \in I } , \mathbb { P } )$ as well as an $\mathbb { R } ^ { d }$ -valued adapted process $X = ( X ( t ) ) _ { t \in I }$ . If $I = \mathbb { R } _ { + }$ , we assume $( \mathcal { F } _ { t } ) _ { t \in I }$ to be right-continuous. Suppose that the components $X _ { m + 1 } ( t ) , \hdots , X _ { d } ( t )$ are observable whereas $X _ { 1 } ( t ) , \ldots , X _ { m } ( t )$ are not. We let the subscript o stand for the observable part of a vector $\boldsymbol { x } \in \mathbb { R } ^ { d }$ and let $H : = ( \delta _ { m + i , j } ) _ { i = 1 , \ldots , d - m ; ~ j = 1 , \ldots , d }$ , i.e. $x _ { \scriptscriptstyle 0 } : = H x = ( x _ { m + 1 } , \ldots , x _ { d } )$ . For $\Sigma \in \mathbb { R } ^ { d \times d }$ we set $\Sigma _ { : , 0 } : = \Sigma H ^ { \top } = \Sigma _ { 1 : d , m + 1 : d } , \Sigma _ { 0 : } : = H \Sigma = \Sigma _ { m + 1 : d , 1 : d }$ as well as $\Sigma _ { \circ } : = H \Sigma H ^ { \top } = \Sigma _ { m + 1 : d , m + 1 : d } .$ The subscript $\mathbf { u }$ standing for the unobservable part of a vector is treated in the same manner.

We suppose that $\mathbb { E } ( \| X ( t ) \| ^ { 2 } ) < \infty$ for $t \in I$ and consider the following general filtering problem for fixed $t \in I$ . The goal is to minimise the mean square error $\mathbb { E } ( \| X ( t ) - Y \| ^ { 2 } )$ over all random variables $Y$ that are measurable with respect to the observable information

$$
\mathcal { G } _ { t } : = \sigma \big ( \big \{ X _ { \circ } ( s ) : s \in I , s \leq t \big \} \big ) .
$$

We call the minimiser of (4.1) the optimal filter for $X$ . Regardless of any specific model the optimal filter is then given by the conditional mean $\widehat { X } ( t , t ) : = \mathbb { E } ( X ( t ) | \mathcal { G } _ { t } )$ .

# 4.1 Discrete-time linear filtering problems

Let $I = \mathbb { N }$ . For Gaussian state space models, the optimal filter can be computed recursively:

Proposition 4.1 (Kálmán filter). Suppose that $X$ is a linear Gaussian state space model as in Definition 3.1 and set $C ( t ) : = B ( t ) B ( t ) ^ { \top }$ . Let $\widehat { X } ( 0 , - 1 ) : = \mathbb { E } ( X ( 0 ) ) , \widehat { \Sigma } ( 0 , - 1 ) : = \mathrm { C o v } ( X ( 0 ) )$ and

$$
\begin{array} { r l } & { \widehat { X } ( t + 1 , t ) : = a ( t + 1 ) + A ( t + 1 ) \widehat { X } ( t , t ) , } \\ & { \qquad \widehat { X } ( t , t ) : = \widehat { X } ( t , t - 1 ) + \widehat { \Sigma } _ { : , 0 } ( t , t - 1 ) \widehat { \Sigma } _ { 0 } ( t , t - 1 ) ^ { + } \big ( X _ { 0 } ( t ) - \widehat { X } _ { 0 } ( t , t - 1 ) \big ) , } \\ & { \widehat { \Sigma } ( t + 1 , t ) : = A ( t + 1 ) \widehat { \Sigma } ( t , t ) A ( t + 1 ) ^ { \top } + C ( t + 1 ) , } \\ & { \qquad \widehat { \Sigma } ( t , t ) : = \widehat { \Sigma } ( t , t - 1 ) - \widehat { \Sigma } _ { : , 0 } ( t , t - 1 ) \widehat { \Sigma } _ { 0 } ( t , t - 1 ) ^ { + } \widehat { \Sigma } _ { 0 ; : } ( t , t - 1 ) } \end{array}
$$