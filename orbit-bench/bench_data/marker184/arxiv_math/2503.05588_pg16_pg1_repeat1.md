that  $I_t$  is surjective. Suppose  $Z \in L^2(Y, t, d')$  so that  $||Z_n - Z||_2 \to 0$  for some  $Z_n \in \mathcal{E}(Y, t, d')$ . By isometry we conclude that  $H_n := I_t^{-1}(Z_n)$  is a Cauchy sequence converging to some  $H \in L^2(X, t, d')$ . Finally  $||I_tH - Z|| = \lim_{n \to \infty} ||Z_n - Z|| = 0$  and therefore  $I_t(H) = Z$ .

Equation (3.11) is evident in the case that  $\gamma$  is a simple function. For general  $\gamma \in L_t(X, d')$ , choose a sequence of simple  $\gamma^{(n)}$  with  $\|\gamma^{(n)} - \gamma\|_2 \to 0$ . By (3.9) and Itō's isometry we have

$$\left\| \int_0^t \gamma^{(n)}(s) \, \mathrm{d}X(s) - \int_0^t \gamma(s) \, \mathrm{d}X(s) \right\|_2 \to 0, \qquad \left\| \int_0^t \gamma^{(n)}(s) \, \mathrm{d}Y(s) - \int_0^t \gamma(s) \, \mathrm{d}Y(s) \right\|_2 \to 0$$

as  $n \to \infty$ . The claim then follows because the continuity of the operator  $I_t$  yields

$$I_t \int_0^t \gamma(s) \, \mathrm{d}X(s) = \lim_{n \to \infty} I_t \int_0^t \gamma^{(n)}(s) \, \mathrm{d}X(s) = \lim_{n \to \infty} \int_0^t \gamma^{(n)}(s) \, \mathrm{d}Y(s) = \int_0^t \gamma(s) \, \mathrm{d}Y(s). \quad \Box$$

## Filtering, smoothing, and prediction $\mathbf{4}$

This section is devoted to optimal linear filtering, prediction and smoothing of partially observed polynomial processes. We let either  $I := \mathbb{N}$  or  $I := \mathbb{R}_+$  and fix a probability space  $(\Omega, \mathscr{F}, (\mathscr{F}_t)_{t\in I}, \mathbb{P})$  as well as an  $\mathbb{R}^d$ -valued adapted process  $X = (X(t))_{t\in I}$ . If  $I = \mathbb{R}_+$ , we assume  $(\mathscr{F}_t)_{t\in I}$  to be right-continuous. Suppose that the components  $X_{m+1}(t), \ldots, X_d(t)$  are observable whereas  $X_1(t), \ldots, X_m(t)$  are not. We let the subscript o stand for the observable part of a vector  $x \in \mathbb{R}^d$  and let  $H := (\delta_{m+i,j})_{i=1,\dots,d-m; \ j=1,\dots,d}$ , i.e.  $x_0 := Hx = (x_{m+1},\dots,x_d)$ . For  $\Sigma \in \mathbb{R}^{d \times d}$  we set  $\Sigma_{:,\mathsf{o}} := \Sigma H^\top = \Sigma_{1:d,\,m+1:d}, \Sigma_{\mathsf{o},:} := H\Sigma = \Sigma_{m+1:d,\,1:d} \text{ as well as } \Sigma_{\mathsf{o}} := H\Sigma H^\top = \Sigma_{m+1:d,\,m+1:d}. \text{ The } \Sigma_{\mathsf{o},:} := H\Sigma H^\top = \Sigma_{m+1:d,\,m+1:d} \text{ and } \Sigma_{\mathsf{o},:} := H\Sigma H^\top = \Sigma_{m+1:d,\,m+1:d} \text{ and } \Sigma_{\mathsf{o},:} := H\Sigma H^\top = \Sigma_{m+1:d,\,m+1:d} \text{ and } \Sigma_{\mathsf{o},:} := H$ subscript u standing for the unobservable part of a vector is treated in the same manner.

We suppose that  $\mathbb{E}(\|X(t)\|^2) < \infty$  for  $t \in I$  and consider the following general filtering problem for fixed *t* ∈ *I*. The goal is to minimise the mean square error  $\mathbb{E}(\|X(t) - Y\|^2)$  over all random variables  $Y$  that are measurable with respect to the observable information

$$\mathscr{G}_t := \sigma\big(\big\{X_\mathrm{o}(s) : s \in I, \ s \le t\big\}\big). \tag{4.1}$$

We call the minimiser of  $(4.1)$  the optimal filter for X. Regardless of any specific model the optimal filter is then given by the conditional mean  $\widehat{X}(t,t) := \mathbb{E}(X(t)|\mathscr{G}_t)$ .

## $4.1$ **Discrete-time linear filtering problems**

Let  $I = \mathbb{N}$ . For Gaussian state space models, the *optimal filter* can be computed recursively:

**Proposition 4.1** (Kálmán filter). Suppose that  $X$  is a linear Gaussian state space model as in Definition 3.1 and set  $C(t) := B(t)B(t)^{\top}$ . Let  $\widehat{X}(0, -1) := \mathbb{E}(X(0)), \widehat{\Sigma}(0, -1) := \text{Cov}(X(0))$  and

$$\begin{split}\n\widehat{X}(t+1,t) &:= a(t+1) + A(t+1)\widehat{X}(t,t), \\
\widehat{X}(t,t) &:= \widehat{X}(t,t-1) + \widehat{\Sigma}_{:,o}(t,t-1)\widehat{\Sigma}_{o}(t,t-1)^{+} \big(X_{o}(t) - \widehat{X}_{o}(t,t-1)\big) \\
\widehat{\Sigma}(t+1,t) &:= A(t+1)\widehat{\Sigma}(t,t)A(t+1)^{\top} + C(t+1), \\
\widehat{\Sigma}(t,t) &:= \widehat{\Sigma}(t,t-1) - \widehat{\Sigma}_{:,o}(t,t-1)\widehat{\Sigma}_{o}(t,t-1)^{+} \widehat{\Sigma}_{o,:}(t,t-1)\n\end{split}$$