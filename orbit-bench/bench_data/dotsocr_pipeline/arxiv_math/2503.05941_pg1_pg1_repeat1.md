[{"bbox": [37, 457, 94, 1410], "category": "Text", "text": "arXiv:2503.05941v1 [math.OC] 7 Mar 2025"}, {"bbox": [375, 185, 1219, 282], "category": "Title", "text": "# Choosing Augmentation Parameters in OSQP- A New Approach based on Conjugate Directions"}, {"bbox": [512, 325, 1081, 360], "category": "Text", "text": "Avinash Kumar (Email: avishimpu@gmail.com)"}, {"bbox": [136, 416, 783, 702], "category": "Text", "text": "*Abstract*— This work proposes a new method to select the augmentation parameters in the operator splitting quadratic program (OSQP) algorithm so as to reduce the computation time of overall algorithm. The selection is based upon the information of conjugate directions of the coefficient matrix of a linear system of equations present in the algorithm. This selection makes it possible to cache these conjugate directions, instead of computing them at each iteration, resulting in faster computation of the solution of the linear system thus reducing the overall computation time. This reduction is demonstrated by a numerical example."}, {"bbox": [136, 703, 783, 950], "category": "Text", "text": "**Notation:** Throughout this manuscript, scalars and scalar-valued functions are denoted by small ordinary alphabets, vectors are denoted by bold ordinary alphabets, and matrices are denoted by capital alphabets. $X \\succ 0$ means that the matrix $X$ is symmetric positive definite. $diag(\\bullet)$ represents a diagonal matrix. A '$T$' in the superscript represents the transpose. '$k$' in the superscript of a quantity denotes the value of the quantity at the $k$th iteration."}, {"bbox": [355, 971, 562, 1000], "category": "Section-header", "text": "## I. INTRODUCTION"}, {"bbox": [136, 1013, 783, 1415], "category": "Text", "text": "The analysis of convex optimization problems is of great importance in applied mathematics. Convex optimization problems arise in numerous fields including - machine learning, control engineering (model predictive control), Lasso and Huber fitting and so on [1]. With the advent of big data and consequently higher dimensional data availability, the size of the convex optimization problems that are required to be solved continues to grow in size. Therefore, to handle real-time optimization-based tasks, it becomes necessary to develop algorithms that can work efficiently and effectively with the high-dimensional data. Furthermore, study of the solution algorithms for convex optimization problems turns out fruitful for the analysis of non-convex problems."}, {"bbox": [136, 1415, 783, 1477], "category": "Text", "text": "In this work, we consider the convex optimization problem, called quadratic program, of the form as stated below."}, {"bbox": [299, 1490, 781, 1593], "category": "Formula", "text": "$$ \\begin{array}{ll} \\text{minimize} & \\left(\\frac{1}{2}\\right) x^T P x + q^T x \\\\ \\text{subject to} & l \\le Ax \\le u \\end{array} \\qquad (1) $$"}, {"bbox": [136, 1610, 783, 1797], "category": "Text", "text": "where $x \\in \\mathbb{R}^n$ is the decision variable, $P \\in \\mathbb{R}^{n \\times n}$ is a positive semi-definite matrix, $q \\in \\mathbb{R}^n$, $A \\in \\mathbb{R}^{m \\times n}$, and $l \\in \\mathbb{R}^m$ and $u \\in \\mathbb{R}^m$ define the lower and upper limits on the variable $Ax$. This formulation allows handling equality constraints by setting $l_i = u_i$. There exist a plethora of algorithms to solve the optimization problem [1] and can"}, {"bbox": [812, 414, 1457, 661], "category": "Text", "text": "be broadly classified into three categories: (1) active-set methods [2], [3], (2) interior-point methods [4], and (3) first-order methods [5]. The major drawback of active set methods is that their worst-case complexity grows exponentially with the number of constraints [6] while the interior point methods are not scalable for very large scale problems because each iteration requires heavy computational tasks to be performed [7]."}, {"bbox": [812, 661, 1457, 1711], "category": "Text", "text": "In recent years, the alternating direction method of multipliers (ADMM), which is an operator splitting based first-order method, has received a lot of attention by the researchers. This is because this method seems to be very well suited for the large scale problems because of the decomposability (inherited from the dual ascent algorithm) and superior convergence properties (inherited from the method of multipliers) [8]. Moreover, the steps involved in the ADMM algorithm are computationally cheap and simple to implement [1]. However, the optimal selection of the parameters, inherited from the augmented Lagrangian, involved in the ADMM algorithm is still an open problem [8]. Operator Splitting Quadratic Program (OSQP), proposed in [1], is one of the most popular general-purpose solvers for the convex quadratic programs. The algorithm is based on a novel splitting technique and subsequent usage of the auxiliary variables leading to a quasi-definite linear system which is always solvable. One of the most computationally expensive step in the OSQP algorithm involves the solution of a linear system-a common trait of the algorithms derived from ADMM. It is this linear system which we propose to solve in a more efficient manner. To do so we use the information about the *conjugate directions* of the coefficient matrix. The information about the conjugate directions is of grave importance in the study of convex optimization. To the best of author's knowledge, this line of research, where the information of the conjugate directions is exploited to improve the characteristics of the algorithm, remains unexplored in the literature and the author believes that it holds promise. The information about the conjugate directions allows us to compute offline a set of parameters and cache them thus avoiding their computation at each iteration of the algorithm leading to reduction in the computation time of the overall algorithm which is demonstrated by a numerical example."}, {"bbox": [812, 1711, 1457, 1896], "category": "Text", "text": "The rest of the paper is organised as follows. Section II presents a review of the OSQP solver as reported in [1] followed by the background of conjugate direction and conjugate gradient methods. In Section III, we present the method to choose augmentation parameters by utilising the information of the conjugate directions of a specific coef-"}, {"bbox": [136, 1823, 781, 1896], "category": "Footnote", "text": "*Acknowledgement:* The author is thankful to Prof. Hoai Nam Nguyen (Télécom SudParis) for the fruitful discussions while working at Télécom SudParis, Institut Polytechnique de Paris."}]