[{"bbox": [171, 178, 1325, 354], "category": "Text", "text": "The paper is organized as follows: In section 2, we introduce Bayesian inversion framework and the Fisher information adaptive MALA. We outline the finite-dimensional Bayesian approach, describe the Fisher adaptive MALA process, and develop the convergence analysis. Section 3 presents numerical experiments for Bayesian inverse problems that demonstrate the effectiveness of this algorithm and compare its performance with other methods. Section 4 concludes the paper with a summary of the main findings and a discussion of potential directions for future research."}, {"bbox": [171, 393, 1029, 436], "category": "Section-header", "text": "# 2 Fisher Adaptive MALA in Bayesian inversion"}, {"bbox": [171, 455, 1325, 517], "category": "Text", "text": "In this section, we review the Bayesian approach to inverse problems. Then, we apply the Fisher adaptive MALA to sample from the Bayesian posterior distributions arising in inverse problems."}, {"bbox": [171, 548, 685, 585], "category": "Section-header", "text": "## 2.1 Bayesian inversion framework"}, {"bbox": [171, 596, 1325, 657], "category": "Text", "text": "Throughout this work, we consider the inverse problem of finding an unknown parameter $x \\in \\mathbb{R}^d$ from data $y \\in \\mathbb{R}^n$, where the relationship between $x$ and $y$ is described by the following model"}, {"bbox": [644, 677, 1323, 709], "category": "Formula", "text": "$$y = \\mathcal{F}(x) + \\eta, \\qquad (2.1)$$"}, {"bbox": [171, 726, 1325, 848], "category": "Text", "text": "where $\\mathcal{F}: \\mathbb{R}^d \\to \\mathbb{R}^n$ is referred as forward operator, and $\\eta$ represents the measurement noise, which is assumed to follow an $n$-dimensional Gaussian distribution. In the Bayesian framework, the vectors $x$, $\\eta$ and $y$ are treated as random variables. Assuming that $x$ and $\\eta$ are independent with $\\pi_0$ as the prior distribution and $\\eta \\sim N(0, \\Sigma)$ as the Gaussian noise. The joint distribution of $(x, y)$ is expressed as"}, {"bbox": [619, 867, 875, 899], "category": "Formula", "text": "$$\\pi(x, y) = \\pi(y | x)\\pi_0(x).$$"}, {"bbox": [171, 917, 1323, 978], "category": "Text", "text": "Furthermore, applying Bayes' rule, the posterior distribution of the unknown parameter $x$ from the observed data $y$ is given by"}, {"bbox": [511, 974, 982, 1037], "category": "Formula", "text": "$$\\pi(x | y) = \\frac{\\pi(y|x)\\pi_0(x)}{Z(y)} \\propto \\exp(-\\Phi(x))\\pi_0(x),$$"}, {"bbox": [171, 1046, 673, 1077], "category": "Text", "text": "where, $Z(y)$ is the normalization constant and"}, {"bbox": [449, 1092, 1323, 1154], "category": "Formula", "text": "$$\\Phi(x) := \\frac{1}{2} \\|\\mathcal{F}(x) - y\\|_{\\Sigma}^{2} = \\frac{1}{2} (\\mathcal{F}(x) - y)^{\\top} \\Sigma^{-1} (\\mathcal{F}(x) - y), \\qquad (2.2)$$"}, {"bbox": [171, 1167, 1323, 1227], "category": "Text", "text": "is the data fidelity term. For the prior distribution $\\pi_0(x)$, we primarily assume a Gaussian prior with zero mean and covariance $C$, i.e. $\\pi_0(x) = N(0, C)$ and"}, {"bbox": [508, 1243, 984, 1305], "category": "Formula", "text": "$$\\pi_0(x) \\propto \\exp\\{-\\frac{1}{2}\\|x\\|_C^2\\} = \\exp\\{-\\frac{1}{2}x^\\top C^{-1}x\\}.$$"}, {"bbox": [171, 1316, 734, 1349], "category": "Text", "text": "Thus the posterior distribution can be expressed as:"}, {"bbox": [565, 1364, 1323, 1424], "category": "Formula", "text": "$$\\pi(x | y) \\propto \\exp\\{-\\Phi(x) - \\frac{1}{2}\\|x\\|_C^2\\}. \\qquad (2.3)$$"}, {"bbox": [171, 1437, 1323, 1470], "category": "Text", "text": "Let $\\pi(x) = \\pi(x | y)$ (here omit the conditioning of data $y$), then the gradient of the log posterior is given by"}, {"bbox": [369, 1485, 1323, 1521], "category": "Formula", "text": "$$\\nabla \\log \\pi(x) = -C^{-1}x - \\nabla \\Phi(x) \\text{ and } \\nabla \\Phi(x) = (\\nabla \\mathcal{F}(x))^{\\top} \\Sigma^{-1} (\\mathcal{F}(x) - y), \\qquad (2.4)$$"}, {"bbox": [171, 1539, 651, 1570], "category": "Text", "text": "where $\\nabla \\mathcal{F}(x)$ is the Fr√©chet derivative of $x$."}, {"bbox": [171, 1568, 1323, 1659], "category": "Text", "text": "A special case appears if $\\mathcal{F}(x)$ is a linear mapping, such as $\\mathcal{F}(x) = Fx$. Then it is known from [19] that posterior distribution is Gaussian, i.e. $\\pi(x | y) = N(\\mu_{post}, C_{post})$ and the mean $\\mu_{post}$ and covariance matrix $C_{post}$ are given by"}, {"bbox": [462, 1653, 1323, 1689], "category": "Formula", "text": "$$C_{post} = (C^{-1} + F^{\\top}\\Sigma^{-1}F)^{-1}, \\quad \\mu_{post} = C_{post}F^{\\top}\\Sigma^{-1}y, \\qquad (2.5)$$"}, {"bbox": [171, 1699, 218, 1728], "category": "Text", "text": "and"}, {"bbox": [601, 1724, 1323, 1760], "category": "Formula", "text": "$$\\nabla \\Phi(x) = F^{\\top}\\Sigma^{-1}(Fx - y). \\qquad (2.6)$$"}]